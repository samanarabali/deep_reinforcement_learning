{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okoPN2VMS6xY"
      },
      "source": [
        "#### Deep Reinforcement Learning\n",
        "\n",
        "\n",
        "A simple DRL model that finds the closest path to the center of an integer plane regardless of where it starts. The allowed actions are left, right, up, down. The positions $(x,y)$ are integer and step = 1 in each direction. A good action receives a +1 reward while a bad action receives a -1 reward.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YzRtbMr82Cf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from collections import deque\n",
        "from keras.activations import relu, linear\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "from datetime import datetime\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmtd3j4zDSDB"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "  def __init__(self, length = 6, width = 6, x = [] ,y = []):\n",
        "    self.length = length\n",
        "    self.width = width\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def reset(self):\n",
        "    self.x = random.randint(0, self.length)\n",
        "    self.y = random.randint(0, self.width)\n",
        "    return [self.x, self.y]\n",
        "\n",
        "\n",
        "  def step(self, received_action):\n",
        "\n",
        "    # previous distance\n",
        "    prev_distance = np.sqrt((self.x - 3)**2 + ((self.y - 3)**2))\n",
        "\n",
        "    # received_action is an int variable (0,1,2,3)\n",
        "    if received_action == 0:\n",
        "      self.x = max(self.x - 1, 0) # move left\n",
        "    elif received_action == 1:\n",
        "      self.x = min(self.x + 1, self.width)  # move right\n",
        "    elif received_action == 2:\n",
        "      self.y = max(self.y - 1, 0) # move down\n",
        "    elif received_action == 3:\n",
        "      self.y = min(self.y + 1, self.length) # move up\n",
        "\n",
        "    # next distance\n",
        "    next_distance = np.sqrt((self.x - 3)**2 + ((self.y - 3)**2))\n",
        "\n",
        "    if prev_distance > next_distance:\n",
        "      reward = 1 # good action\n",
        "    else:\n",
        "      reward = -1 # bad action\n",
        "\n",
        "    next_state = [self.x, self.y]\n",
        "\n",
        "    done = False\n",
        "    if next_distance <= 0.1:\n",
        "      done = True\n",
        "      reward = 5\n",
        "\n",
        "    info = []\n",
        "\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "\n",
        "  def plot(self):\n",
        "    fig = plt.figure()\n",
        "    # fig.set_size_inches(self.length, self.width) # Set the size of the figure\n",
        "    plt.xlim(-0.1, self.width + 0.1)\n",
        "    plt.ylim(-0.1, self.length + 0.1)\n",
        "    plt.scatter(self.x, self.y, s=15, c =\"blue\") # Plot the data\n",
        "    plt.show() # Show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk_CgOAHSV46"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.counter = 0\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.density_first_layer = 512\n",
        "        self.density_second_layer = 256\n",
        "        self.num_epochs = 3\n",
        "        self.batch_size = 128\n",
        "        self.epsilon_min = 0.01\n",
        "\n",
        "        # epsilon function parameters\n",
        "        self.epsilon = 0.7\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Learning rate\n",
        "        self.lr = 0.001\n",
        "\n",
        "        self.rewards_list = []\n",
        "        self.replay_memory_buffer = deque(maxlen=20000)\n",
        "        # num_action_space refers to the number of possible actions an agent can take in a given state (here: move up, down, left, right)\n",
        "        self.num_action_space = 4\n",
        "\n",
        "        # num_observation_space refers to the set of possible values an agent can observe from the environment at each time step\n",
        "        self.num_observation_space = 2 # (x,y)\n",
        "\n",
        "        self.model = self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.density_first_layer, input_dim=self.num_observation_space, activation=relu))\n",
        "        model.add(Dense(self.density_second_layer, activation=relu))\n",
        "        model.add(Dense(self.num_action_space, activation=linear))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=self.lr))\n",
        "        print(model.summary())\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"The epsilon parameter decides whether we use Q-function to generate the next action or\n",
        "        a random action\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.randrange(self.num_action_space)\n",
        "            # rand_act = random.randrange(self.num_action_space)\n",
        "            # return [0 if i != rand_act else 1 for i in range(0,4)]\n",
        "\n",
        "        # Get a list of predictions based on the current state\n",
        "        predicted_actions = self.model.predict(state)\n",
        "\n",
        "        # Return the maximum-reward action\n",
        "        return np.argmax(predicted_actions[0])\n",
        "\n",
        "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def update_weights(self):\n",
        "\n",
        "        # Check if we have higher than batch_size actions in the buffer\n",
        "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
        "            return\n",
        "\n",
        "        # Choose batch of random samples from the memory\n",
        "        random_sample = self.get_random_sample_from_memory()\n",
        "\n",
        "        # Get the values from the random batch of samples\n",
        "        states, actions, rewards, next_states, done_list = self.get_values_from_samples(random_sample)\n",
        "\n",
        "        # Q-function to estimate the targets using the random batch of next states\n",
        "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
        "\n",
        "        # Run a prediction on the states in our random sample\n",
        "        target_vec = self.model.predict_on_batch(states)\n",
        "\n",
        "        # Create indices for the batch_size\n",
        "        indexes = np.array([i for i in range(self.batch_size)])\n",
        "\n",
        "        # The target vector is an array of state predictions\n",
        "        target_vec[[indexes], [actions]] = targets\n",
        "\n",
        "        # Train the model with the existing states and target scores and update the weights\n",
        "        self.model.fit(states, target_vec, epochs=self.num_epochs, verbose=0)\n",
        "\n",
        "    def get_values_from_samples(self, random_sample):\n",
        "        states = np.array([i[0] for i in random_sample])\n",
        "        actions = np.array([i[1] for i in random_sample])\n",
        "        rewards = np.array([i[2] for i in random_sample])\n",
        "        next_states = np.array([i[3] for i in random_sample])\n",
        "        done_list = np.array([i[4] for i in random_sample])\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
        "\n",
        "    # Get a batch_size sample of previous iterations\n",
        "    def get_random_sample_from_memory(self):\n",
        "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "        return random_sample\n",
        "\n",
        "    # Run the keras predict using the current state as input to find the next step\n",
        "    def predict(self, current_state):\n",
        "        return self.model.predict(current_state)\n",
        "\n",
        "    def train(self, num_episodes=2000, can_stop=True):\n",
        "\n",
        "        frames = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "\n",
        "            # state is a vector of (x,y) position\n",
        "            state = env.reset()\n",
        "            reward_for_episode = 0\n",
        "            done = False\n",
        "            state = np.reshape(state, [1, self.num_observation_space])\n",
        "            while not done:\n",
        "\n",
        "                # store the states\n",
        "                frames.append(state)\n",
        "\n",
        "                # use epsilon decay to choose the next state\n",
        "                received_action = self.get_action(state)\n",
        "                next_state, reward, done, info = env.step(received_action)\n",
        "\n",
        "                # Reshape the next_state array to match the size of the observation space\n",
        "                next_state = np.reshape(next_state, [1, self.num_observation_space])\n",
        "\n",
        "                # Store the experience in replay memory\n",
        "                self.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
        "\n",
        "                # add up rewards\n",
        "                reward_for_episode += reward\n",
        "                state = next_state\n",
        "                self.update_counter()\n",
        "\n",
        "                # update the model\n",
        "                self.update_weights()\n",
        "\n",
        "            self.rewards_list.append(reward_for_episode)\n",
        "\n",
        "            # Decay the epsilon after each experience completion\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "\n",
        "            # Calculate the average reward\n",
        "            last_rewards_mean = np.mean(self.rewards_list[-100:])\n",
        "\n",
        "            print(episode, \"\\t: Episode || Reward: \",reward_for_episode, \"\\t|| Average Reward: \",last_rewards_mean, \"\\t epsilon: \", self.epsilon)\n",
        "\n",
        "            # Once the episode number reaches episode_number, finish the training\n",
        "            if episode == num_episodes -1:\n",
        "                self.states_log = frames\n",
        "                print(\"DQN Training Complete...\")\n",
        "                break\n",
        "\n",
        "    def update_counter(self):\n",
        "        self.counter += 1\n",
        "        step_size = 2\n",
        "        self.counter = self.counter % step_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idQZmuFTKS7R"
      },
      "outputs": [],
      "source": [
        "# Testing \n",
        "# x = [i[0][0] for i in model.states_log]\n",
        "# y = [i[0][1] for i in model.states_log]\n",
        "\n",
        "# # x = [0, 1, 2, 3, 4]\n",
        "# # y = [0, 1, 2, 3, 4]\n",
        "# Env(x = x,y = y).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv7ayDf69sng"
      },
      "outputs": [],
      "source": [
        "if __name__==\"__main__\":\n",
        "    rewards_list = []\n",
        "\n",
        "    # Create the Enironment\n",
        "    env = Env()\n",
        "\n",
        "    # set the random generator seed\n",
        "    np.random.seed(21)\n",
        "\n",
        "    # max number of training episodes\n",
        "    training_episodes = 1000\n",
        "\n",
        "    # initialize the Deep-Q Network model\n",
        "    model = DQN(env)\n",
        "\n",
        "    # Train the model\n",
        "    model.train(training_episodes, True)\n",
        "\n",
        "    now = datetime.now() # current date and time\n",
        "    date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # save the model\n",
        "    model.model.save('/tmp/mymodel-' + date_time + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbxdJrCEYpLq"
      },
      "outputs": [],
      "source": [
        "if __name__==\"__main__\":\n",
        "\n",
        "    print(\"Starting Testing of the trained model...\")\n",
        "\n",
        "    it = 0 # number of iterations\n",
        "\n",
        "    # Run x episodes\n",
        "    num_test_episode = 1\n",
        "\n",
        "    # number of test runs\n",
        "    high_score = 0\n",
        "\n",
        "    # Create the Enironment\n",
        "    env = Env()\n",
        "    rewards_list = []\n",
        "\n",
        "    # Load the model\n",
        "    mymodel = load_model(\"mymodel-20240204-055800.h5\")\n",
        "    done = False\n",
        "    frames = []\n",
        "\n",
        "    # Run some test episodes to see how well our model performs\n",
        "    for test_episode in range(num_test_episode):\n",
        "        current_state = env.reset()\n",
        "        num_observation_space = 2\n",
        "        current_state = np.reshape(current_state, [1, num_observation_space])\n",
        "        reward_for_episode = 0\n",
        "        done = False\n",
        "        while (not done) and (it <= 20):\n",
        "            frame = current_state\n",
        "            frames.append(frame)\n",
        "            it += 1\n",
        "\n",
        "            selected_action = np.argmax(mymodel.predict(current_state)[0])\n",
        "            new_state, reward, done, info = env.step(selected_action)\n",
        "            new_state = np.reshape(new_state, [1, num_observation_space])\n",
        "            current_state = new_state\n",
        "            reward_for_episode += reward\n",
        "        rewards_list.append(reward_for_episode)\n",
        "        it = 0 # reset\n",
        "        print(test_episode, \"\\t: Episode || Reward: \", reward_for_episode)\n",
        "        frame = current_state\n",
        "        frames.append(frame)\n",
        "\n",
        "    rewards_mean = np.mean(rewards_list[-100:])\n",
        "    print(\"Average Reward: \", rewards_mean )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPF5MqK0EZa_"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "x = [i[0][0] for i in frames]\n",
        "y = [i[0][1] for i in frames]\n",
        "\n",
        "Env(x = x,y = y).plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
